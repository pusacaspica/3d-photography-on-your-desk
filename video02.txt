Boa noite, meu nome é Horácio Macêdo, e junto do colega Issufi, ficamos de implementar o trabalho escrito por Bougie e Perona em 1998 no trabalho 3d photography on your desk onde, usando apenas uma fonte de luz, uma sombra e um objeto para projetar sombra, cria-se um scanner 3d. Este vídeo está sendo regravado porque coisas aconteceram nesta semana que passou, o que tornou o último vídeo obsoleto.

No formulário anterior, nós falamos que um bom indicativo de andamento do trabalho a curto prazo seria ter posse de uma nuvem de pontos, e o objetivo a longo prazo seria a obtenção da malha final. Nós realmente subestimamos o trabalho que a gente teria, e as dificuldades que encontraríamos, mas depois desta última semana, eu acho que estamos num lugar melhor do que eu achei que estivéssemos originalmente.

No atual momento do trabalho, nós temos o mapeamento espaço-temporal de sombras, que a gente compreende ser a parte principal do trabalho. Depois de um mês entendendo como usar ndarrays direito para que o processamento não demorasse horas inteiras, nós conseguimos implementar funções que nos entrega tanto as coordenadas da sombra em cada frame do vídeo, como também o tempo em que a sombra intercepta cada pixel da imagem. Também coletamos informações necessárias para calibração de câmeras, que deve dar início agora que a dúvida da semana passada foi sanada e, ao mesmo tempo, abriu toda uma família nova de dúvidas que provavelmente são respondidas em algum dos livros-textos. O artigo de Tsai, ao mesmo tempo em que deu uma boa ideia do processo envolvido ao redor de calibração de câmeras, nos deixou com dúvida sobre como usar a matriz de calibração da câmera na real calibração da câmera.

Também há dúvida em como apropriadamente isolar a família de planos usadas para mapeamento dos vértices. Enquanto que o matplotlib nos fornece um conjunto de funções que serão usadas para a reconstrução das malhas, "fazer a intersecção do plano indexado pelo mapeamento temporal de um pixel com o raio ótico" ainda é algo que nos dá alguma dúvida.

Na implementação atual, o mapeamento temporal está definindo coordenadas de vértices de modo que o t é usado como coodenada de profundidade, tomando como referência o plano da imagem. Claramente, os modelos não estão tão legais assim, e a gente meio que só tem uma malha de pontos até agora mesmo.

Sobre as imagens, no atual momento, para mitigar imperfeições do mundo real, estamos trabalhando com imagens geradas pelo software Blender, renderizadas pelo renderizador Cycles. O Blender também representa facilidade na hora de calibrar imagens porque informações que antes seriam extraídas através de DLTs podem ser diretamente acessadas, apesar da dúvida de como usar a matriz de calibração. Nas imagens, estamos usando objetos lambertianos, como recomendado pelo experimento. Chegamos a visitar o laboratório duas vezes, mas o processo de desenvolver o programa nos fez perceber problemas com as fotos produzidas. Nada que uma terceira visita ao laboratório, desta vez usando o projetor maior como fonte de luz direcionada. Das últimas vezes a gente usou a lanterna do celular porque nenhum projetor ficava estável em um ângulo que nos ajudasse, mas desta vez eu provavelmente vou colocar o projetor no meu ombro porque é o dá para fazer.